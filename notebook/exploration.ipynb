{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03b3eb52-f7fb-4f93-a3fa-deb5e68b7cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23b56cc6-6935-4ed6-bba7-a29784cfc465",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"AIzaSyBPPxyjUzyZ-1z5bL3R47igdzKaup9DMsg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3de4cd46-764c-4a16-8ab5-6dd87516f524",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../artifacts_pdf/tables/transformer_article/tbl-0003-00-6321cb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6fbdf49-e0f7-4529-a120-8ed4f1327824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  TheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\n",
      "0  connectedlayersforboththeencoderanddecoder,sho...                                       \n",
      "1                                      respectively.                                       \n",
      "2                                                NaN                                       \n",
      "3                        3.1 EncoderandDecoderStacks                                       \n",
      "4                                                NaN                                       \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e1eff5a-c07f-40db-97bc-eccf0d151220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>connectedlayersforboththeencoderanddecoder,sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>respectively.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  TheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\n",
       "0  connectedlayersforboththeencoderanddecoder,sho...                                       \n",
       "1                                      respectively.                                       \n",
       "2                                                NaN                                       "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace55be9-6020-43ca-aa80-dabfe9a68890",
   "metadata": {},
   "source": [
    "### check images content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d0fb147-2e14-480e-817e-7c0815aaed73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7662a91a-41de-48b3-9ac1-94f758bfb1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\", api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "553d3f64-5bcc-4874-a2b4-5f7d4321a2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = Path(r\"C:\\Users\\ozzyg\\Documents\\mydocs\\1_work\\RAG_MULTI_DOC\")\n",
    "chroma_dir = project_root / \"chroma_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88553fc5-a618-43ff-8455-6159648e0b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = Chroma(\n",
    "    collection_name=\"pdf_images\",\n",
    "    persist_directory=chroma_dir,\n",
    "    embedding_function=emb,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2d68542-2aa6-4a4a-88ca-69c86f48cb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = store.get()  # get all or use where={\"pdf_path\": \"data/transformer_article.pdf\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d81062a-a5b3-407c-93fd-5b0b38b4e258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['bdfaa68d8984f0dc:image:p0001:fef97f8ba684',\n",
       "  'bdfaa68d8984f0dc:image:p0002:813d3a39b9df',\n",
       "  'bdfaa68d8984f0dc:image:p0003:3d1eea41fb4f',\n",
       "  'bdfaa68d8984f0dc:image:p0003:ff5ab8feb78f'],\n",
       " 'embeddings': None,\n",
       " 'documents': ['This image is a research paper titled \"Attention Is All You Need\" by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. The paper introduces a new network architecture called the Transformer, which relies solely on attention mechanisms and achieves state-of-the-art results on machine translation tasks.',\n",
       "  \"This excerpt is from a research paper that introduces the Transformer model for sequence transduction, highlighting its reliance on attention mechanisms for global dependencies between input and output, enabling more parallelization and improved translation quality. The paper also discusses the Transformer's background, model architecture, and advantages over other models.\",\n",
       "  'This image is a diagram of the Transformer model architecture, consisting of an encoder on the left and a decoder on the right, each with multiple layers of multi-head attention and feed-forward networks. It shows how inputs are processed through embedding, positional encoding, and repeated layers to produce output probabilities.',\n",
       "  'This image shows a transformer architecture with input and output embeddings, positional encoding, and multi-head attention layers. The model uses feed-forward networks, add and norm layers, and linear and softmax layers to produce output probabilities.'],\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'documents'],\n",
       " 'data': None,\n",
       " 'metadatas': [{'file_name': 'transformer_article.pdf',\n",
       "   'pdf_path': 'data\\\\transformer_article.pdf',\n",
       "   'bbox_sig': '0_0_612_792',\n",
       "   'bbox_x1': 612.0,\n",
       "   'type': 'image',\n",
       "   'bbox_y0': 0.0,\n",
       "   'has_caption': True,\n",
       "   'has_ocr': False,\n",
       "   'bbox_y1': 792.0,\n",
       "   'bbox_x0': 0.0,\n",
       "   'doc_tag': \"{'doc_hash': 'bdfaa68d8984f0dc02beaca527b76f207d99b666d31d1da728ee0728182df697', 'file_name': 'transformer_article.pdf'}\",\n",
       "   'page': 1,\n",
       "   'doc_hash': 'bdfaa68d8984f0dc',\n",
       "   'image_id': 'fig-p0001-full-35f7fa',\n",
       "   'png_path': 'artifacts_pdf\\\\figures\\\\transformer_article\\\\fig-p0001-full-35f7fa.png'},\n",
       "  {'image_id': 'fig-p0002-full-277931',\n",
       "   'pdf_path': 'data\\\\transformer_article.pdf',\n",
       "   'doc_tag': \"{'doc_hash': 'bdfaa68d8984f0dc02beaca527b76f207d99b666d31d1da728ee0728182df697', 'file_name': 'transformer_article.pdf'}\",\n",
       "   'has_ocr': False,\n",
       "   'type': 'image',\n",
       "   'bbox_y1': 792.0,\n",
       "   'has_caption': True,\n",
       "   'bbox_x0': 0.0,\n",
       "   'page': 2,\n",
       "   'png_path': 'artifacts_pdf\\\\figures\\\\transformer_article\\\\fig-p0002-full-277931.png',\n",
       "   'bbox_sig': '0_0_612_792',\n",
       "   'file_name': 'transformer_article.pdf',\n",
       "   'bbox_y0': 0.0,\n",
       "   'doc_hash': 'bdfaa68d8984f0dc',\n",
       "   'bbox_x1': 612.0},\n",
       "  {'doc_tag': \"{'doc_hash': 'bdfaa68d8984f0dc02beaca527b76f207d99b666d31d1da728ee0728182df697', 'file_name': 'transformer_article.pdf'}\",\n",
       "   'bbox_x0': 0.0,\n",
       "   'bbox_y1': 792.0,\n",
       "   'file_name': 'transformer_article.pdf',\n",
       "   'doc_hash': 'bdfaa68d8984f0dc',\n",
       "   'png_path': 'artifacts_pdf\\\\figures\\\\transformer_article\\\\fig-p0003-full-482c44.png',\n",
       "   'bbox_x1': 612.0,\n",
       "   'pdf_path': 'data\\\\transformer_article.pdf',\n",
       "   'image_id': 'fig-p0003-full-482c44',\n",
       "   'has_ocr': False,\n",
       "   'has_caption': True,\n",
       "   'page': 3,\n",
       "   'type': 'image',\n",
       "   'bbox_sig': '0_0_612_792',\n",
       "   'bbox_y0': 0.0},\n",
       "  {'page': 3,\n",
       "   'type': 'image',\n",
       "   'bbox_sig': 'none',\n",
       "   'image_id': 'fig-p0003-img00-1b836b',\n",
       "   'has_caption': True,\n",
       "   'pdf_path': 'data\\\\transformer_article.pdf',\n",
       "   'has_ocr': False,\n",
       "   'png_path': 'artifacts_pdf\\\\figures\\\\transformer_article\\\\fig-p0003-img00-1b836b.png',\n",
       "   'doc_hash': 'bdfaa68d8984f0dc',\n",
       "   'file_name': 'transformer_article.pdf',\n",
       "   'doc_tag': \"{'doc_hash': 'bdfaa68d8984f0dc02beaca527b76f207d99b666d31d1da728ee0728182df697', 'file_name': 'transformer_article.pdf'}\"}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70d6a869-2135-471c-86be-033b8c6d156b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FILE: None\n",
      "CONTENT: This image is a research paper titled \"Attention Is All You Need\" by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. The paper introduces a new network architecture called the Transformer, which relies solely on attention mechanisms and achieves state-of-the-art results on machine translation tasks.\n",
      "\n",
      "FILE: None\n",
      "CONTENT: This excerpt is from a research paper that introduces the Transformer model for sequence transduction, highlighting its reliance on attention mechanisms for global dependencies between input and output, enabling more parallelization and improved translation quality. The paper also discusses the Transformer's background, model architecture, and advantages over other models.\n",
      "\n",
      "FILE: None\n",
      "CONTENT: This image is a diagram of the Transformer model architecture, consisting of an encoder on the left and a decoder on the right, each with multiple layers of multi-head attention and feed-forward networks. It shows how inputs are processed through embedding, positional encoding, and repeated layers to produce output probabilities.\n",
      "\n",
      "FILE: None\n",
      "CONTENT: This image shows a transformer architecture with input and output embeddings, positional encoding, and multi-head attention layers. The model uses feed-forward networks, add and norm layers, and linear and softmax layers to produce output probabilities.\n"
     ]
    }
   ],
   "source": [
    "for meta, doc in zip(docs[\"metadatas\"], docs[\"documents\"]):\n",
    "    print(\"\\nFILE:\", meta.get(\"source\"))\n",
    "    print(\"CONTENT:\", doc[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c3de5c-4257-4325-9dfc-04d6a831a9db",
   "metadata": {},
   "source": [
    "### pdf function - text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30024ba3-faff-4244-bfd0-d2f7b054a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Iterable, Optional, Sequence\n",
    "import logging\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfadae06-1c50-4632-ac5a-d93702ce2bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_pdf_text(pdf_path: str, *, num_pages: Optional[int] = None) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load a PDF into a list of LangChain Document objects (one per page).\n",
    "    If num_pages is provided, return only the first N pages.\n",
    "    \"\"\"\n",
    "    pages = PyPDFLoader(pdf_path).load()\n",
    "    if not num_pages or num_pages <= 0:\n",
    "        return pages\n",
    "    return pages[:min(num_pages, len(pages))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33c3d69d-05ec-4e26-b3f9-5d293d48e607",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = (\"../data/transformer_article.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c43cbfa0-0d08-4a9e-bb4f-6ed9da2e1115",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_text = load_pdf_text(pdf_path, num_pages =3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ee1ed23-13c0-434c-98bc-699741289db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/transformer_article.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani‚àó\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer‚àó\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar‚àó\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit‚àó\\nGoogle Research\\nusz@google.com\\nLlion Jones‚àó\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez‚àó ‚Ä†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\n≈Åukasz Kaiser‚àó\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin‚àó ‚Ä°\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n‚àóEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n‚Ä†Work performed while at Google Brain.\\n‚Ä°Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca89cff-360f-4c26-9914-fd0303185ee6",
   "metadata": {},
   "source": [
    "### pdf function - table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd04bcc4-f6de-4eaf-a926-9079cd447b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/utils/tables_extractor.py\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple\n",
    "import csv\n",
    "import json\n",
    "import uuid\n",
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2272c4c8-7b0f-47db-8cbc-cbceb9044bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "BBox = Tuple[float, float, float, float]\n",
    "\n",
    "#\n",
    "@dataclass\n",
    "class TableArtifact:\n",
    "    table_id: str\n",
    "    pdf_path: str\n",
    "    page: int                 # 1-based\n",
    "    bbox: Optional[BBox]      # (x0, y0, x1, y1) in PDF points, if available\n",
    "    title: Optional[str]      # best-effort title guess, None for MVP\n",
    "    headers: List[str]\n",
    "    rows: List[List[str]]     # full table rows\n",
    "    row_count: int\n",
    "    csv_path: str             # saved CSV file\n",
    "    json_path: str            # saved JSON summary\n",
    "    preview_text: str         # what you will embed/index in Chroma\n",
    "\n",
    "\n",
    "def _clean_cell(x):\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    x = str(x).strip()\n",
    "    return x\n",
    "\n",
    "\n",
    "def _make_preview_text(title: Optional[str], headers: List[str], rows: List[List[str]]) -> str:\n",
    "    r1 = rows[0] if rows else []\n",
    "    r2 = rows[1] if len(rows) > 1 else []\n",
    "    title_txt = f\"Table: {title}.\" if title else \"Table.\"\n",
    "    return (\n",
    "        f\"{title_txt} Headers: \" + \" | \".join(headers) +\n",
    "        (f\". Example rows: \" + \" | \".join(r1) if r1 else \"\") +\n",
    "        (f\" ; \" + \" | \".join(r2) if r2 else \"\")\n",
    "    )\n",
    "\n",
    "\n",
    "def extract_tables(\n",
    "    pdf_path: str,\n",
    "    out_dir: str = \"artifacts_pdf\",\n",
    "    *,\n",
    "    num_pages: Optional[int] = None,     # <-- NEW: if 3, take the first 3 pages only\n",
    ") -> List[TableArtifact]:\n",
    "    \"\"\"\n",
    "    MVP table extractor using pdfplumber. Returns TableArtifact list and writes CSV+JSON artifacts.\n",
    "\n",
    "    Args:\n",
    "        pdf_path: path to the PDF\n",
    "        out_dir: base directory for artifacts\n",
    "        num_pages: if provided and >=1, only the first N pages are processed (1..N)\n",
    "    \"\"\"\n",
    "    pdf_path_p = Path(pdf_path)\n",
    "    assert pdf_path_p.exists(), f\"PDF not found: {pdf_path}\"\n",
    "\n",
    "    out_base = Path(out_dir) / \"tables1\" / pdf_path_p.stem\n",
    "    out_base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    artifacts: List[TableArtifact] = []\n",
    "\n",
    "    with pdfplumber.open(str(pdf_path_p)) as pdf:\n",
    "        total = len(pdf.pages)\n",
    "        limit = total if not num_pages or num_pages <= 0 else min(num_pages, total)\n",
    "\n",
    "        for pi in range(limit):   # 0-based index of the first N pages\n",
    "            page_num = pi + 1\n",
    "            page = pdf.pages[pi]\n",
    "\n",
    "            # Two simple strategies: \"lines\" (ruled tables) then \"text\" (whitespace)\n",
    "            strategies = [\n",
    "                {\"vertical_strategy\": \"lines\", \"horizontal_strategy\": \"lines\", \"explicit_vertical_lines\": None},\n",
    "                {\"vertical_strategy\": \"text\", \"horizontal_strategy\": \"text\"},\n",
    "            ]\n",
    "\n",
    "            found_any = False\n",
    "            for settings in strategies:\n",
    "                try:\n",
    "                    tables = page.find_tables(table_settings=settings)  # returns Table objects\n",
    "                except Exception:\n",
    "                    tables = []\n",
    "\n",
    "                if not tables:\n",
    "                    continue\n",
    "\n",
    "                for t_idx, t in enumerate(tables):\n",
    "                    try:\n",
    "                        rows = [[_clean_cell(c) for c in row] for row in t.extract()]\n",
    "                        if not rows:\n",
    "                            continue\n",
    "\n",
    "                        # Heuristic: first row as headers, remaining as data\n",
    "                        headers = [h for h in rows[0]]\n",
    "                        data_rows = rows[1:] if len(rows) > 1 else []\n",
    "\n",
    "                        table_id = f\"tbl-{page_num:04d}-{t_idx:02d}-{uuid.uuid4().hex[:6]}\"\n",
    "                        csv_path = out_base / f\"{table_id}.csv\"\n",
    "                        json_path = out_base / f\"{table_id}.json\"\n",
    "\n",
    "                        # Save CSV (headers + data)\n",
    "                        with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "                            w = csv.writer(f)\n",
    "                            if headers:\n",
    "                                w.writerow(headers)\n",
    "                            for r in data_rows:\n",
    "                                w.writerow(r)\n",
    "\n",
    "                        # Build preview text for embedding\n",
    "                        preview_text = _make_preview_text(None, headers, data_rows[:2])\n",
    "\n",
    "                        art = TableArtifact(\n",
    "                            table_id=table_id,\n",
    "                            pdf_path=str(pdf_path_p),\n",
    "                            page=page_num,\n",
    "                            bbox=tuple(t.bbox) if getattr(t, \"bbox\", None) else None,\n",
    "                            title=None,\n",
    "                            headers=headers,\n",
    "                            rows=data_rows,\n",
    "                            row_count=len(data_rows),\n",
    "                            csv_path=str(csv_path),\n",
    "                            json_path=str(json_path),\n",
    "                            preview_text=preview_text,\n",
    "                        )\n",
    "\n",
    "                        # Save JSON sidecar (metadata + first few rows to keep it small)\n",
    "                        with open(json_path, \"w\", encoding=\"utf-8\") as jf:\n",
    "                            json.dump({\n",
    "                                **asdict(art),\n",
    "                                \"rows\": data_rows[:10],\n",
    "                            }, jf, ensure_ascii=False, indent=2)\n",
    "\n",
    "                        artifacts.append(art)\n",
    "                        found_any = True\n",
    "                    except Exception:\n",
    "                        # swallow table-level errors; keep going\n",
    "                        continue\n",
    "\n",
    "                if found_any:\n",
    "                    break  # don't run next strategy if this one already found tables\n",
    "\n",
    "    return artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ffba5ce-fbfa-4119-8142-9901910e29a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = extract_tables(\n",
    "    pdf_path,\n",
    "    num_pages = 6, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d2d69ae-545a-4257-b5d2-743b714a73cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.TableArtifact"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tables[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "418157b0-cad5-4ff8-af0f-f399d656d1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"TableArtifact(table_id: 'str', pdf_path: 'str', page: 'int', bbox: 'Optional[BBox]', title: 'Optional[str]', headers: 'List[str]', rows: 'List[List[str]]', row_count: 'int', csv_path: 'str', json_path: 'str', preview_text: 'str')\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tables[5]).__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f189545-a2ea-46b1-a16b-bf2fd3beea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_dict = asdict(tables[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "348b3bb2-e9dc-4564-b0d1-3aa2c0297f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Table. Headers: Table1: Maximumpathlengths,per-layercomplexityandminimumnumberofsequentialoperations. Example rows: fordifferentlayertypes. nisthesequencelength,distherepresentationdimension,kisthekernel ; sizeofconvolutionsandrthesizeoftheneighborhoodinrestrictedself-attention.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_dict['preview_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "287e4607-badf-44b2-bd0b-bafe1d2d6c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tbl-0006-00-280a44'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_dict['table_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d040b5c7-7dfb-471e-9484-bab27d57dccf",
   "metadata": {},
   "source": [
    "### pdf function - image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0bbd1b27-527d-47d2-b111-0840a0fbd891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Callable, List, Optional, Tuple, TYPE_CHECKING\n",
    "import uuid\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from src.clients.gemini_client import GeminiClient  # for type hints only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c7b33ac1-a3db-4c9d-bc2f-08ccf4753736",
   "metadata": {},
   "outputs": [],
   "source": [
    "BBox = Tuple[float, float, float, float]\n",
    "\n",
    "@dataclass\n",
    "class ImageArtifact:\n",
    "    image_id: str\n",
    "    pdf_path: str\n",
    "    page: int                 # 1-based\n",
    "    bbox: Optional[BBox]      # None for embedded images when bbox unknown\n",
    "    png_path: str\n",
    "    caption: str\n",
    "    ocr_text: Optional[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "201b2e5a-ed28-4f2a-9b9c-b14bc5beb452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Optional captioner using YOUR GeminiClient (injectable) -----\n",
    "# Signature: captioner(png_bytes: bytes, *, mime: str) -> str\n",
    "def caption_with_gemini_client_factory(\n",
    "    client: \"GeminiClient\",                          # forward-ref to avoid import cycles\n",
    "    prompt: str = \"Describe this image in 1‚Äì2 sentences.\",\n",
    "    model: str = \"gemini-2.0-flash\",\n",
    "    **vision_kwargs,\n",
    ") -> Callable[[bytes], str]:\n",
    "    \"\"\"\n",
    "    Returns a function that accepts image bytes and returns a short caption\n",
    "    using your GeminiClient.generate_vision().\n",
    "    \"\"\"\n",
    "    def _caption(png_bytes: bytes, *, mime: str = \"image/png\") -> str:\n",
    "        try:\n",
    "            result = client.generate_vision(\n",
    "                user_prompt=prompt,\n",
    "                images=[png_bytes],     # bytes OK\n",
    "                mime=mime,\n",
    "                model=model,\n",
    "                **vision_kwargs\n",
    "            )\n",
    "            if isinstance(result, str):\n",
    "                return result.strip()\n",
    "            if result is None:\n",
    "                return \"\"\n",
    "            return str(result).strip()\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "    return _caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "66403068-252e-49b0-8d99-b9c15f966068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _try_ocr(png_path: Path) -> Optional[str]:\n",
    "    try:\n",
    "        import pytesseract\n",
    "        text = pytesseract.image_to_string(Image.open(png_path))\n",
    "        return text.strip()[:2000]  # keep small\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2727de1f-9177-469a-8563-6733432a97a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mime_from_ext(ext: str) -> str:\n",
    "    ext = (ext or \"\").lower().lstrip(\".\")\n",
    "    return {\n",
    "        \"png\": \"image/png\",\n",
    "        \"jpg\": \"image/jpeg\",\n",
    "        \"jpeg\": \"image/jpeg\",\n",
    "        \"webp\": \"image/webp\",\n",
    "        \"tif\": \"image/tiff\",\n",
    "        \"tiff\": \"image/tiff\",\n",
    "        \"bmp\": \"image/bmp\",\n",
    "        \"gif\": \"image/gif\",\n",
    "    }.get(ext, \"image/png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6a6cda8e-4652-4244-86e6-5afd4e454b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images(\n",
    "    pdf_path: str,\n",
    "    out_dir: str = \"artifacts_pdf\",\n",
    "    dpi: int = 144,\n",
    "    run_ocr: bool = False,\n",
    "    captioner: Optional[Callable[..., str]] = None,   # uses captioner(img_bytes, mime=...)\n",
    "    save_fullpage: bool = False,\n",
    "    extract_embedded: bool = True,\n",
    "    *,\n",
    "    num_pages: Optional[int] = None,                  # <-- NEW: take only the first N pages\n",
    ") -> List[ImageArtifact]:\n",
    "    \"\"\"\n",
    "    MVP image extractor.\n",
    "    - Renders each page to PNG (full-page figure) for stable citations.\n",
    "    - Optionally extracts embedded images too (if present).\n",
    "    - Optionally captions via GeminiClient and runs OCR.\n",
    "    - If num_pages is provided (>=1), only the first N pages are processed.\n",
    "    \"\"\"\n",
    "    pdf_path_p = Path(pdf_path)\n",
    "    assert pdf_path_p.exists(), f\"PDF not found: {pdf_path}\"\n",
    "\n",
    "    out_base = Path(out_dir) / \"figures\" / pdf_path_p.stem\n",
    "    out_base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    artifacts: List[ImageArtifact] = []\n",
    "\n",
    "    doc = fitz.open(str(pdf_path_p))\n",
    "    try:\n",
    "        total = len(doc)\n",
    "        limit = total if not num_pages or num_pages <= 0 else min(num_pages, total)\n",
    "\n",
    "        for pi in range(limit):  # 0-based\n",
    "            page = doc[pi]\n",
    "            page_num = pi + 1\n",
    "\n",
    "            # 1) Full-page render (robust fallback that \"sees\" charts and vector drawings)\n",
    "            if save_fullpage:\n",
    "                m = fitz.Matrix(dpi / 72.0, dpi / 72.0)\n",
    "                pix = page.get_pixmap(matrix=m, alpha=False)\n",
    "                image_id = f\"fig-p{page_num:04d}-full-{uuid.uuid4().hex[:6]}\"\n",
    "                png_path = out_base / f\"{image_id}.png\"\n",
    "                pix.save(str(png_path))\n",
    "\n",
    "                caption = \"\"\n",
    "                if captioner:\n",
    "                    caption = captioner(Path(png_path).read_bytes(), mime=\"image/png\") or \"\"\n",
    "                ocr_text = _try_ocr(png_path) if run_ocr else None\n",
    "\n",
    "                rect = page.rect  # (x0, y0, x1, y1)\n",
    "                artifacts.append(ImageArtifact(\n",
    "                    image_id=image_id,\n",
    "                    pdf_path=str(pdf_path_p),\n",
    "                    page=page_num,\n",
    "                    bbox=(rect.x0, rect.y0, rect.x1, rect.y1),\n",
    "                    png_path=str(png_path),\n",
    "                    caption=caption,\n",
    "                    ocr_text=ocr_text,\n",
    "                ))\n",
    "\n",
    "            # 2) Embedded raster images (if any). Note: bbox is not provided by PyMuPDF here.\n",
    "            if extract_embedded:\n",
    "                for img_idx, img in enumerate(page.get_images(full=True)):\n",
    "                    try:\n",
    "                        xref = img[0]\n",
    "                        base_image = doc.extract_image(xref)\n",
    "                        ext = base_image.get(\"ext\", \"png\")\n",
    "                        image_bytes = base_image[\"image\"]\n",
    "                        image_id = f\"fig-p{page_num:04d}-img{img_idx:02d}-{uuid.uuid4().hex[:6]}\"\n",
    "                        file_path = out_base / f\"{image_id}.{ext}\"\n",
    "                        with open(file_path, \"wb\") as f:\n",
    "                            f.write(image_bytes)\n",
    "\n",
    "                        mime = _mime_from_ext(ext)\n",
    "                        caption = \"\"\n",
    "                        if captioner:\n",
    "                            caption = captioner(image_bytes, mime=mime) or \"\"\n",
    "                        ocr_text = _try_ocr(file_path) if run_ocr else None\n",
    "\n",
    "                        artifacts.append(ImageArtifact(\n",
    "                            image_id=image_id,\n",
    "                            pdf_path=str(pdf_path_p),\n",
    "                            page=page_num,\n",
    "                            bbox=None,\n",
    "                            png_path=str(file_path),\n",
    "                            caption=caption,\n",
    "                            ocr_text=ocr_text,\n",
    "                        ))\n",
    "                    except Exception:\n",
    "                        continue  # be resilient\n",
    "    finally:\n",
    "        doc.close()\n",
    "\n",
    "    return artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9410db-179e-4ace-ada7-34c30d457108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3632350-bd3f-411e-850a-bef75c71db24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb24b194-eb9d-4991-a1da-0d0ceb25ef02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gcp)",
   "language": "python",
   "name": "gcp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
