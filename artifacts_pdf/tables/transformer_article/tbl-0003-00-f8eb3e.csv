"TheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully"
"connectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,"
respectively.
""
3.1 EncoderandDecoderStacks
""
Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two
"sub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-"
wisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof
"the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is"
"LayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer"
"itself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding"
""
"layers,produceoutputsofdimensiond =512.
model"
""
Decoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo
"sub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head"
"attentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections"
"aroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention"
sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This
"masking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe"
predictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.
""
3.2 Attention
""
"Anattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,"
"wherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum"
