1 Introduction
""
"Recurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks"
"inparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand"
"transductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous"
effortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder
"architectures[38,24,15]."
""
Recurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput
"sequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden"
""
"statesh ,asafunctionoftheprevioushiddenstateh andtheinputforpositiont. Thisinherently
t tâˆ’1"
"sequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger"
"sequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved"
significantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional
"computation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental"
"constraintofsequentialcomputation,however,remains."
""
Attentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-
"tionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein"
"theinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms"
areusedinconjunctionwitharecurrentnetwork.
""
"InthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead"
relyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.
TheTransformerallowsforsignificantlymoreparallelizationandcanreachanewstateoftheartin
translationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.
""
2 Background
""
ThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU
"[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding"
"block,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,"
thenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows
"inthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes"
it more difficult to learn dependencies between distant positions [12]. In the Transformer this is
"reducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue"
"to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as"
describedinsection3.2.
""
"Self-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions"
ofasinglesequenceinordertocomputearepresentationofthesequence. Self-attentionhasbeen
"usedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,"
"textualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22]."
""
End-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-
alignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand
languagemodelingtasks[34].
""
"To the best of our knowledge, however, the Transformer is the first transduction model relying"
entirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-
"alignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate"
"self-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9]."
""
3 ModelArchitecture
""
"Mostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35]."
""
"Here, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence
1 n"
""
"of continuous representations z = (z ,...,z ). Given z, the decoder then generates an output
1 n"
""
"sequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive
1 m"
"[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext."
