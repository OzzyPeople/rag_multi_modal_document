,"Providedproperattributionisprovided,Googleherebygrantspermissionto"
,reproducethetablesandfiguresinthispapersolelyforuseinjournalisticor
,scholarlyworks.
,
,Attention Is All You Need
,
32,
0,AshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗
2,GoogleBrain GoogleBrain GoogleResearch GoogleResearch
g,avaswani@google.com noam@google.com nikip@google.com usz@google.com
u,
A,LlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗
2,GoogleResearch UniversityofToronto GoogleBrain
,llion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com
]L,
C,IlliaPolosukhin∗ ‡
.,illia.polosukhin@gmail.com
sc[,
,
,
7,Abstract
v,
2,Thedominantsequencetransductionmodelsarebasedoncomplexrecurrentor
67,convolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest
3,performing models also connect the encoder and decoder through an attention
0,"mechanism. We propose a new simple network architecture, the Transformer,"
.,"basedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions"
6,entirely. Experiments on two machine translation tasks show these models to
07,besuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly
1,less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-
:,"to-German translation task, improving over the existing best results, including"
v,"ensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,"
iX,ourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after
r,"trainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe"
a,bestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto
,othertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith
,largeandlimitedtrainingdata.
,
,∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted
,"theefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand"
,"hasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head"
,attentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvolvedinnearlyevery
,"detail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand"
,"tensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and"
,efficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand
,"implementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating"
