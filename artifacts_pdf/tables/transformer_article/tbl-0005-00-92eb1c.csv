"output values. These are concatenated and once again projected, resulting in the final values, as"
depictedinFigure2.
""
Multi-headattentionallowsthemodeltojointlyattendtoinformationfromdifferentrepresentation
"subspacesatdifferentpositions. Withasingleattentionhead,averaginginhibitsthis."
""
O
"MultiHead(Q,K,V)=Concat(head ,...,head )W
1 h"
""
Q K V
"wherehead =Attention(QW ,KW ,VW )
i i i i"
""
WQ ∈Rdmodel×dk WK ∈Rdmodel×dk WV ∈Rdmodel×dv
"Wheretheprojectionsareparametermatrices , ,
i i i"
andWO ∈Rhdv×dmodel.
""
"In this work we employ h = 8 parallel attention layers, or heads. For each of these we use"
""
"d =d =d /h=64. Duetothereduceddimensionofeachhead,thetotalcomputationalcost
k v model"
issimilartothatofsingle-headattentionwithfulldimensionality.
""
3.2.3 ApplicationsofAttentioninourModel
""
TheTransformerusesmulti-headattentioninthreedifferentways:
""
"• In""encoder-decoderattention""layers,thequeriescomefromthepreviousdecoderlayer,"
andthememorykeysandvaluescomefromtheoutputoftheencoder. Thisallowsevery
positioninthedecodertoattendoverallpositionsintheinputsequence. Thismimicsthe
typical encoder-decoder attention mechanisms in sequence-to-sequence models such as
"[38,2,9]."
""
"• Theencodercontainsself-attentionlayers. Inaself-attentionlayerallofthekeys,values"
"andqueriescomefromthesameplace,inthiscase,theoutputofthepreviouslayerinthe"
encoder. Eachpositionintheencodercanattendtoallpositionsinthepreviouslayerofthe
encoder.
""
"• Similarly,self-attentionlayersinthedecoderalloweachpositioninthedecodertoattendto"
allpositionsinthedecoderuptoandincludingthatposition. Weneedtopreventleftward
informationflowinthedecodertopreservetheauto-regressiveproperty. Weimplementthis
insideofscaleddot-productattentionbymaskingout(settingto−∞)allvaluesintheinput
ofthesoftmaxwhichcorrespondtoillegalconnections. SeeFigure2.
""
3.3 Position-wiseFeed-ForwardNetworks
""
"Inadditiontoattentionsub-layers,eachofthelayersinourencoderanddecodercontainsafully"
"connectedfeed-forwardnetwork,whichisappliedtoeachpositionseparatelyandidentically. This"
consistsoftwolineartransformationswithaReLUactivationinbetween.
""
""
"FFN(x)=max(0,xW +b )W +b (2)
1 1 2 2"
""
"Whilethelineartransformationsarethesameacrossdifferentpositions,theyusedifferentparameters"
from layer to layer. Another way of describing this is as two convolutions with kernel size 1.
""
"The dimensionality of input and output is d = 512, and the inner-layer has dimensionality
model"
""
"d =2048.
ff"
""
3.4 EmbeddingsandSoftmax
""
"Similarlytoothersequencetransductionmodels,weuselearnedembeddingstoconverttheinput"
""
"tokensandoutputtokenstovectorsofdimensiond . Wealsousetheusuallearnedlineartransfor-
model"
mationandsoftmaxfunctiontoconvertthedecoderoutputtopredictednext-tokenprobabilities. In
"ourmodel,wesharethesameweightmatrixbetweenthetwoembeddinglayersandthepre-√softmax"
"lineartransformation,similarto[30]. Intheembeddinglayers,wemultiplythoseweightsby d .
model"
